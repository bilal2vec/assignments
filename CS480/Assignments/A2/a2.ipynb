{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a231e54",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "- This assignment was posted on 30 September, 2024 and is due on ~~18 October 2024~~ 21 October 2024, at 11:59 pm\n",
    "- Answer all questions in this Jupyter notebook skeleton within the provided cells. Questions will indicate whether the answer should take the form of a coded or written response. Use the dropdown menu within the Jupyter interface to toggle between 'Markdown' or 'Code' for the cells. Do NOT delete or rearrange any of the question blocks within this skeleton.\n",
    "- The following two files should be submitted to LEARN:\n",
    "    - This IPYNB file containing the questions and your answers in either code or markdown.\n",
    "    - A PDF printout of this IPYNB file. To generate this, first run and save the output of all cells. Then expand all cells and print as PDF. Be sure that all your code and answers are visible in the PDF document you submit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453a6605",
   "metadata": {},
   "source": [
    "## Exercise 1. Maximum Likelihood Estimation\n",
    "\n",
    "The probability density function of a Beta distribution is given by:\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{\\eta},\n",
    "$$\n",
    "\n",
    "where $x$ is the value of the random variable, $\\alpha$ and $\\beta$ are shape parameters, and $\\eta$ is a normalizing constant. Suppose that a dataset $\\boldsymbol{x} = \\{ x_1, x_2, \\ldots, x_n \\}$ is given, where each observation $x_i$ follows a Beta distribution. It is known that the relationship between the shape parameters is:\n",
    "\n",
    "$$\n",
    "\\beta = \\alpha^2\n",
    "$$\n",
    "\n",
    "### **EXERCISE1-TASK1: [10 marks]**\n",
    "\n",
    "Derive the maximum likelihood estimate of the shape parameter $\\beta$. That is, starting with the likelihood function, $\\mathcal{L}(\\alpha, \\beta \\mid \\boldsymbol{x})$, provide an analytical expression to compute $\\beta$, given some $\\alpha$.\n",
    "The normalizing constant $\\eta$ can be omitted during differentiation, as it does not affect the location of the maximum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6bf480",
   "metadata": {},
   "source": [
    "### Write your solution here using MathJax and Markdown ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b3dc2",
   "metadata": {},
   "source": [
    "## Exercise 2. Kernel Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a160902",
   "metadata": {},
   "source": [
    "### **EXERCISE2-TASK1: [5 marks]**\n",
    "\n",
    "Perform Kernel (Parzen) Density estimation on the Mystery.csv dataset in the folder, using your own gaussian kernel implemented from scratch.\n",
    "\n",
    "Detailed instructions: First, define a function `gaussian_kernel(x, xp, ls, variance=1)` that takes as inputs: `x`, a scalar or an array of points where the kernel is evaluated; `xp`, a scalar or an array of data points; `ls`, the length scale (or bandwidth) of the Gaussian kernel; and `variance`, the variance of the Gaussian kernel (which can be set to a default of 1). The function should compute and return the Gaussian kernel value as:\n",
    "\n",
    "$$\n",
    "K(x, x') = \\frac{1}{\\sqrt{2 \\pi \\text{variance}}} \\exp\\left(- \\frac{(x - x')^2}{2 \\cdot \\text{variance} \\cdot \\text{ls}^2}\\right).\n",
    "$$\n",
    "\n",
    "After implementing the Gaussian kernel function, perform Kernel Density Estimation (KDE) on the dataset. Use a length scale of 0.05. To do this, define 100 evaluation points uniformly over the range [0,1], and compute the empirical density at each evaluation point. Finally, create a scatter plot of the sample data points along the 1D line and plot the estimated density using the evaluation points. Label the axes appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f801bfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### YOUR SOLUTION GOES HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27456127",
   "metadata": {},
   "source": [
    "### **EXERCISE2-TASK2: [5 marks]**\n",
    "\n",
    "Implement the Kullback-Leibler (KL) divergence and use it to compare the empirical distribution from Part 1 against two fitted parametric distributions: the Beta distribution and the Gaussian distribution.\n",
    "\n",
    "Detailed instructions: Start by defining a function `compute_kl_divergence(ps, qs, epsilon=1e-3)` that computes the empirical KL divergence between two probability distributions evaluated at a set of points over the domain: `ps` (the reference distribution) and `qs` (the model distribution). Recall that the KL divergence is computed as\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P \\parallel Q) = \\sum_x P(x) \\log\\left(\\frac{P(x)}{Q(x)}\\right).\n",
    "$$\n",
    "\n",
    "To avoid issues with division by zero or logarithm of zero, clip values from below by a small constant `epsilon` set to 1e-3. This ensures numerical stability in the computation. Next, estimate the parameters of the sample data using `scipy.stats.beta.fit()` to obtain a fitted Beta distribution, and fit a Gaussian distribution to the data using the sample mean and standard deviation. Evaluate your fitted parametric models at the same evaluation points that you used in Part 1. Be sure to normalize each distribution (empirical, Beta, and Gaussian) so that the sum of the probabilities over the evaluation points is equal to 1 before computing the KL divergence. Using the KL divergence function, compute the KL divergence between the empirical distribution (KDE from Part 1) and each of the fitted Beta and Gaussian distributions. Print out the KL divergence values for comparison.\n",
    "\n",
    "Based on the KL divergence values, which parametric distribution (Beta or Gaussian) better approximates the empirical distribution? Provide one sentence to interpret the results. Include a plot showing the empirical KDE, the fitted Beta distribution, and the fitted Gaussian distribution to support your explanation. Make sure to label all axes and provide a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a372aceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION GOES HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865f1912",
   "metadata": {},
   "source": [
    "The beta distribution fits the data better as it resulted in a lower KL divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ffd96",
   "metadata": {},
   "source": [
    "## Exercise 3. Logistic Regression\n",
    "\n",
    "### **EXERCISE3-TASK1: [5 marks]**\n",
    "\n",
    "Implement a logistic regression model and a function to compute the gradient of the cross-entropy loss with respect to the parameters. \n",
    "\n",
    "Detailed instructions: Start by defining a function `predict_prob(w, x)` that computes the predicted probability of the positive class for a given feature vector `x` and weight vector `w`. Recall that in logistic regression, the predicted probability is given by: \n",
    "\n",
    "$$\n",
    "\\hat{y} = \\frac{1}{1 + e^{-\\langle w, x \\rangle}},\n",
    "$$\n",
    "\n",
    "where $\\langle w, x \\rangle$ is the inner (dot) product between the parameter vector and feature vector. Next, implement a function called `compute\\_gradient(ys, xs, w)` that computes the gradient of the loss function with respect to the weight vector `w`, using the true labels `ys`, and feature vectors `xs`. Define a function for the gradient of the cross-entropy loss, and verify that the implementation is correct by generating a synthetic dataset with a single feature using `numpy` and known parameters. You may find the function `numpy.random.choice` useful. Since the true parameters correspond to the generating distribution of the labels, the gradient should be close to zero at this point. Print the gradient at the true parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad1cf376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### YOUR SOLUTION GOES HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a14e47e",
   "metadata": {},
   "source": [
    "### **EXERCISE3-TASK2: [5 marks]**\n",
    "    \n",
    "Build and train a logistic regression model using gradient descent on the `heart.csv` dataset in the folder, and visualize the loss in successive training epochs. You may use `pandas`, `matplotlib`, and `numpy`.\n",
    "\n",
    "Detailed instructions: \n",
    "Begin by loading the heart disease dataset stored in `heart.csv`. Note that this dataset has categorial columns (\"cp\" and \"\"), that must be converted to one-hot encoded columns. The labels, that indicate the presence or absence of heart disease, are stored in the \"target\" column. Split the dataset into disjoint training set (66%) and a testing set (33%). \n",
    "\n",
    "Implement a function `grad_descent` that performs gradient descent to update the parameter vector. Include options for specifying the learning rate, tolerance for early stopping (if weights stop changing), and maximum number of iterations per training epoch. Consider how you would track convergence and decide when to stop the optimization.\n",
    "\n",
    "Finally, train your model using gradient descent. Use a learning rate of `1e-4`, tolerance of `1e-6`, and a maximum number of iterations per training epoch of 10000. Train your model over 10 iterations, and compute and track the cross-entropy loss on the testing set after each epoch of gradient descent. Use the function `predict_prob` from **EXERCISE3-TASK1** to return the predicted probability for a given set of inputs. Plot the loss values over multiple epochs to visualize how the model converges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1580b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "### YOUR SOLUTION GOES HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12afbb45",
   "metadata": {},
   "source": [
    "## Exercise 4. Stochastic Gradient Descent\n",
    "### **EXERCISE4-TASK1: [10 marks]**\n",
    "Show how the Perceptron algorithm can be interpreted as a particular form of stochastic gradient descent (SGD). \n",
    "In particular, you must provide a loss function and a gradient step size (learning rate) such that SGD with these parameters and the perceptron algorithm sare identical. This question was adapted from Prof. Gautam Kamath, in his Introduction to ML course as taught in Fall 2023. \n",
    "\n",
    "Detailed instructions: Recall that in the Perceptron algorithm, a parameter update occurs only when a sample is misclassified. Determine a loss function that assigns zero loss for correctly classified samples and a positive loss for misclassified samples. The Perceptron algorithm uses a fixed step size (learning rate) for updating the weights whenever a sample is misclassified. Determine what the learning rate should be in your SGD setup so that the resulting parameter updates are identical to those in the Perceptron algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3232c47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your solution here using MathJax and Markdown ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2aa3e",
   "metadata": {},
   "source": [
    "## Exercise 5. Hard-Margin SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13ddf2d",
   "metadata": {},
   "source": [
    "### **EXERCISE5-TASK1: [20 marks]**\n",
    "\n",
    "Implement a Support Vector Machine (SVM) for binary classification using the dual formulation of the hard-margin SVM problem, and apply it to the Iris dataset. \n",
    "\n",
    "Detailed instructions: Begin by loading the `Iris.csv` dataset and filter it to include only two classes, \n",
    "Convert the target variable `variety` to binary labels, where `Setosa` is labeled as `+1` and all other classes are labeled as `-1`. Construct a 66\\%/33\\s% train/test split.\n",
    "You may use methods from `sklearn.preprocessing` to conduct these steps.\n",
    "\n",
    "Next, set up the dual problem for optimization problem using `cvxpy`. This will involve creating a `cvxpy` variable to represent the Lagrange multipliers $\\lambda$, as well as specifying the objective and the constraints. Recall that the dual form of the hard-margin SVM problem is:\n",
    "\n",
    "$$\n",
    "\\max_{\\lambda} \\left( \\sum_{i=1}^{n} \\lambda_i - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\lambda_i \\lambda_j y_i y_j \\langle x_i, x_j \\rangle \\right),\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "\\lambda_i \\geq 0 \\;\\ \\forall i, \\quad \\text{and} \\;\\ \\sum_{i=1}^{n} \\lambda_i y_i = 0,\n",
    "$$\n",
    "\n",
    "where $\\lambda_i$ are the Lagrange multipliers, $y_i$ are the true labels, $x_i$ are the feature vectors, and $\\langle x_i, x_j \\rangle$ is the inner product between feature vectors $x_i$ and $x_j$.\n",
    "To ensure that the matrix used in the quadratic term is positive semi-definite, you may add a small regularization term, to the diagonal elements. Solve the optimization problem using `cvxpy` and extract the Lagrange multipliers from the solution.\n",
    "\n",
    "After obtaining the optimized Lagrange multipliers, compute the weight vector $w$ as a linear combination of the training feature vectors and class labels. Calculate the bias term $b$ using the support vectors.\n",
    "\n",
    "Finally, define a function to make predictions based on the computed weight vector and bias term. Use this function to evaluate the model's performance on the testing set by generating and plotting the confusion matrix as well as computing the ROC AUC score. For evaluation, you may use methods in `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dd56641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CVXPY) Sep 29 11:49:42 PM: Encountered unexpected exception importing solver CVXOPT:\n",
      "ImportError('DLL load failed while importing base: The specified module could not be found.')\n",
      "(CVXPY) Sep 29 11:49:42 PM: Encountered unexpected exception importing solver GLPK:\n",
      "ImportError('DLL load failed while importing base: The specified module could not be found.')\n",
      "(CVXPY) Sep 29 11:49:42 PM: Encountered unexpected exception importing solver GLPK_MI:\n",
      "ImportError('DLL load failed while importing base: The specified module could not be found.')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import cvxpy as cp\n",
    "\n",
    "### YOUR SOLUTION GOES HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e6496d",
   "metadata": {},
   "source": [
    "EXERCISE 6. Feature Embeddings and the Kernel Trick\n",
    "\n",
    "The dataset in the next cell is characterized by two classes, wherein one is nested within the other. This is a classic example of non-linearly separable data in the original feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41be48e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "def generate_nested_hyperspheres(n_samples_per_class, n_features, r1 = 0.1, r2 = .3):\n",
    "    data_xs1 = np.random.normal( size = (n_samples_per_class, n_features) )\n",
    "    data_xs1 /= np.linalg.norm(data_xs1, axis=1,keepdims = True) * r1\n",
    "    data_xs2 = np.random.normal( size = (n_samples_per_class, n_features) )\n",
    "    data_xs2 /= np.linalg.norm(data_xs2, axis = 1,keepdims = True) * r2\n",
    "\n",
    "    # Stack classes and create labels\n",
    "    data_xs = np.concatenate([data_xs1,data_xs2],axis=0)\n",
    "    data_xs += np.random.normal( scale = 1., size = data_xs.shape)\n",
    "    label_ys = np.concatenate([np.ones(n_samples_per_class)*i for i in [-1,1] ])\n",
    "    return data_xs, label_ys\n",
    "\n",
    "data_xs,label_ys = generate_nested_hyperspheres( n_samples_per_class = 10, \n",
    "                                                   n_features = 2)\n",
    "fig,ax = plt.subplots(1,1,figsize=(5.,5.))\n",
    "ax.scatter(data_xs[:,0],data_xs[:,1],c=label_ys,cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb5012",
   "metadata": {},
   "source": [
    "**EXERCISE6-TASK1 [10 marks]**\n",
    "Design a valid feature embedding that transforms the original features into a new space where the classes are linearly separable. \n",
    "\n",
    "Detailed instructions: Implement a function `feature_map(data_xs)` that maps the original features to a (possibly higher-dimensional) space using a custom transformation. Mkae the function general so that it works even if the number of original features changes. You may use feature transformation that you believe will make the data linearly separable in the transformed space. Implement a custom kernel function `kernel(data_xs)` that computes the similarity between data points using your designed feature embedding.\n",
    "Defend your choice of kernel function by referencing the conditions for a valid kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18bb8781",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION GOES HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe281ec2",
   "metadata": {},
   "source": [
    "The **kernel trick** allows us to obtain the result of an inner product between feature vectors in a high-dimensional feature space, without having to explicitly compute the embeddings. \n",
    "\n",
    "**EXERCISE6-TASK2 [ 10 marks]**\n",
    "\n",
    "Experimentally investigate the computational advantages of the kernel trick by comparing the time required to compute the Gram matrix (or kernel matrix) using explicit feature mappings versus implicit kernel computations. \n",
    "\n",
    "Detailed instructions:\n",
    "Use `generate_nested_hyperspheres` to generate synthetic datasets with different numbers of samples (explore 100,200,500,1000,2000) and features (explore 2,5,10,20,50,100,200). \n",
    "Using your feature embedding, measure the time (wall clock time) required to generate the Gram matrix (including feature embedding and generation of the Gram matrix using these features).\n",
    "In parallel, measure the time to generate the Gram matrix implicitly using your kernel function. \n",
    "Create two plots: one that shows how run time depends on the # samples and # features if you explicitly transform the features, and another that shows this same information, but if the Gram is generated using implicit kernel computations. \n",
    "Share the y-axes between the two plots to simplify direct comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4850e9f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# vary the dataset sizes\n",
    "N_SAMPLES = [100, 200, 500, 1000, 2000] \n",
    "N_FEATURES = [2, 5, 10, 20, 50, 100]\n",
    "\n",
    "# FOR storing the timing results\n",
    "timing_results = {\n",
    "    'n_samples': [],\n",
    "    'n_features': [],\n",
    "    'explicit': [],\n",
    "    'implicit': []\n",
    "}\n",
    "\n",
    "### YOUR SOLUTION GOES HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6759e72a",
   "metadata": {},
   "source": [
    "## EXERCISE 7. Cross-Validation\n",
    "\n",
    "### **EXERCISE7-TASK1 [10 marks]**\n",
    "\n",
    "Apply Gaussian Process Regression to the Insurance dataset and select hyperparameters via cross-validation. \n",
    "\n",
    "Detailed instructions: The Insurance dataset contains information about various health factors alongside medical insurance charges. The target variable you will try to predict is this `charges` attribute, given the other features. \n",
    "Split the data into a training set (50%) for cross-validation, as well as a holdout dataset (50%) to provide an unbiased final model evaluation.\n",
    "Apply Z-score scaling to the features and the target before proceeding to fitting/hyperparameter selection, being sure to avoid data leakage in the process. \n",
    "\n",
    "Implement Gaussian Process Regression using the `GaussianProcessRegressor` class from the `sklearn.gaussian_process` module with the `RBF` kernel. For hyperparameter selection, focus on the length scale as well as the regularization (`alpha` parameter) for the RBF kernel. Explore a range of possible length scale values (e.g., `[1.0, 2.0, 5.0, 10.0, 20.0]`) and alpha (e.g. `[0.1,0.2,1.0,2.0,10.0]`), and select the model using 5-fold cross-validation. \n",
    "Recall that this means you should divide the data into 5 equal folds, and for each combination of hyperparameters, fit the model on 4 folds and evaluate it on the remaining fold using mean squared error (MSE) as the evaluation metric.\n",
    "You may NOT use `GridSearchCV` from `sklearn.model_selection` for this part. Be sure to set `n_restarts` to some integer greater than `1` to help find the global optimum. WARNING: This process will take a few minutes, depending on your machine. Consider implementing a progressbar using `tqdm` library so you can move on to others things if you like, while you wait :)\n",
    "\n",
    "Once the hyperparameters have been selected, train your GPR on the full training set (in essence, using ALL folds) and evaluate it on the holdout dataset. Generate a scatter plot of the predicted `charges` scores (on the y-axis) against the true `charges` scores (on the x-axis), for data in the holdout dataset. Print the final MSE on the holdout set and provide a brief comment (1-2 sentences) on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "496f03da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION GOES HERE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
